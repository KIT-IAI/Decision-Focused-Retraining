{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "\n",
    "# Function to download and unzip files\n",
    "def download_and_unzip(url, directory):\n",
    "    \"\"\"_summary_ : This method downloads and unzips the file from the given URL and saves it to the given directory.\n",
    "\n",
    "    Args:\n",
    "        url (_type_): _description_\n",
    "        directory (_type_): _description_\n",
    "    \"\"\"\n",
    "    # Extract the filename from the URL\n",
    "    filename = os.path.join(directory, os.path.basename(url.split(\"?\")[0]))\n",
    "\n",
    "    # Download the file\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(directory)\n",
    "\n",
    "\n",
    "# method to download the ausgrid solar home dataset\n",
    "def download_ausgrid_solar_home_dataset():\n",
    "    \"\"\"_summary_ : Downloads the ausgrid Dataset\"\"\"\n",
    "\n",
    "    # check if the data folder exists\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "    # create the data folder for ausgrid solar home dataset\n",
    "\n",
    "    if not os.path.exists(\"data/ausgrid_solar_home_dataset\"):\n",
    "        os.mkdir(\"data/ausgrid_solar_home_dataset\")\n",
    "        directory = \"data/ausgrid_solar_home_dataset/\"\n",
    "        urls = [\n",
    "            \"https://cdn.ausgrid.com.au/-/media/Documents/Data-to-share/Solar-home-electricity-data/Solar-home-half-hour-data---1-July-2012-to-30-June-2013.zip?rev=de594e37789744738fe747c37e1e67bf\",\n",
    "            \"https://cdn.ausgrid.com.au/-/media/Documents/Data-to-share/Solar-home-electricity-data/Solar-home-half-hour-data---1-July-2011-to-30-June-2012.zip?rev=938d7e42fe0f43969fc4144341dacfac\",\n",
    "            \"https://cdn.ausgrid.com.au/-/media/Documents/Data-to-share/Solar-home-electricity-data/Solar-home-half-hour-data---1-July-2010-to-30-June-2011.zip?rev=3ba8aee669294858a27cda3f2214aba5\",\n",
    "        ]\n",
    "\n",
    "        # Loop through each URL and download/unzip the file\n",
    "        for url in urls:\n",
    "            download_and_unzip(url, directory)\n",
    "\n",
    "\n",
    "def parse_date_and_time_and_get_series(df):\n",
    "    \"\"\"_summary_ : This method parses the date and time and returns a series.\"\"\"\n",
    "    data_and_cols = df[df.columns[4:-1]]\n",
    "    touples = []\n",
    "    for index, row in data_and_cols.iterrows():\n",
    "        initial_date = pd.Timestamp(row.iloc[0])\n",
    "        for row_entry in range(1, len(row)):\n",
    "            tmp_tuple = (\n",
    "                initial_date + row_entry * pd.Timedelta(\"30min\"),\n",
    "                row.iloc[row_entry],\n",
    "            )\n",
    "            touples.append(tmp_tuple)\n",
    "\n",
    "    if len(touples) > 0:\n",
    "        idx, values = zip(*touples)\n",
    "    else:\n",
    "        idx, values = [], []\n",
    "\n",
    "    return pd.Series(values, idx)\n",
    "\n",
    "\n",
    "def generate_facotred_version_of_ausgrid_dataset(consumption_df, prodoction_df,\n",
    "                                                 consumption_factor,\n",
    "                                                 production_factor, suffix):\n",
    "    results_df = consumption_df * consumption_factor - prodoction_df * production_factor\n",
    "    print(\n",
    "        f\"Generated facotred version of the ausgrid dataset with the suffix {suffix}\"\n",
    "    )\n",
    "    # name index time\n",
    "    results_df.index.name = \"time\"\n",
    "    results_df.to_csv(\n",
    "        f\"data/ausgrid_solar_home_dataset/ausgrid_prosumption_{suffix}.csv\")\n",
    "\n",
    "\n",
    "def modify_ausgrid_dataset():\n",
    "    \"\"\"_summary_ : This method reads in the ausgrid solar home dataset and modifies it to a single csv file.\"\"\"\n",
    "\n",
    "    # checks if the data files already exist\n",
    "    if (os.path.exists(\n",
    "            \"data/ausgrid_solar_home_dataset/ausgrid_gc_customers.csv\")\n",
    "            and os.path.exists(\n",
    "                \"data/ausgrid_solar_home_dataset/ausgrid_gg_customers.csv\")\n",
    "            and os.path.exists(\n",
    "                \"data/ausgrid_solar_home_dataset/ausgrid_cl_customers.csv\")\n",
    "            and os.path.exists(\n",
    "                \"data/ausgrid_solar_home_dataset/ausgrid_prosumption.csv\")\n",
    "            and os.path.exists(\n",
    "                \"data/ausgrid_solar_home_dataset/ausgrid_prosumption_ldiv2.csv\"\n",
    "            ) and os.path.exists(\n",
    "                \"data/ausgrid_solar_home_dataset/ausgrid_prosumption_ldiv5.csv\"\n",
    "            ) and os.path.exists(\n",
    "                \"data/ausgrid_solar_home_dataset/ausgrid_prosumption_load5.csv\"\n",
    "            ) and os.path.exists(\n",
    "                \"data/ausgrid_solar_home_dataset/ausgrid_prosumption_load2.csv\"\n",
    "            ) and os.path.exists(\n",
    "                \"data/ausgrid_solar_home_dataset/ausgrid_prosumption_factor5.csv\"\n",
    "            ) and os.path.exists(\n",
    "                \"data/ausgrid_solar_home_dataset/ausgrid_prosumption_factor10.csv\"\n",
    "            )):\n",
    "        return\n",
    "\n",
    "    # read in all 3 ausgrid csv files\n",
    "    ausgrid_2010_2011 = pd.read_csv(\n",
    "        \"data/ausgrid_solar_home_dataset/2010-2011 Solar home electricity data.csv\",\n",
    "        low_memory=False,\n",
    "        skiprows=1,\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    ausgrid_2011_2012 = pd.read_csv(\n",
    "        \"data/ausgrid_solar_home_dataset/2011-2012 Solar home electricity data v2.csv\",\n",
    "        low_memory=False,\n",
    "        skiprows=1,\n",
    "        parse_dates=True,\n",
    "    )\n",
    "    ausgrid_2012_2013 = pd.read_csv(\n",
    "        \"data/ausgrid_solar_home_dataset/2012-2013 Solar home electricity data v2.csv\",\n",
    "        low_memory=False,\n",
    "        skiprows=1,\n",
    "        parse_dates=True,\n",
    "    )\n",
    "\n",
    "    # get the date collumn in uniformae date format\n",
    "    ausgrid_2010_2011[\"date\"] = pd.to_datetime(ausgrid_2010_2011[\"date\"],\n",
    "                                               format=\"mixed\")\n",
    "    ausgrid_2011_2012[\"date\"] = pd.to_datetime(ausgrid_2011_2012[\"date\"],\n",
    "                                               format=\"%d/%m/%Y\")\n",
    "    ausgrid_2012_2013[\"date\"] = pd.to_datetime(ausgrid_2012_2013[\"date\"],\n",
    "                                               format=\"%d/%m/%Y\")\n",
    "\n",
    "    # concatenate the 3 ausgrid datasets\n",
    "    ausgrid = pd.concat(\n",
    "        [ausgrid_2010_2011, ausgrid_2011_2012, ausgrid_2012_2013])\n",
    "\n",
    "    # filter Consumption Category after GC, GG and CL and safe them seperatly\n",
    "    ausgrid_gc = ausgrid[ausgrid[\"Consumption Category\"] == \"GC\"]\n",
    "    ausgrid_gg = ausgrid[ausgrid[\"Consumption Category\"] == \"GG\"]\n",
    "    ausgrid_cl = ausgrid[ausgrid[\"Consumption Category\"] == \"CL\"]\n",
    "\n",
    "    # group them by customer id\n",
    "\n",
    "    # generate a time index from the lowest to the highest date 30 minutes resolution\n",
    "    time_index = pd.date_range(\n",
    "        start=pd.Timestamp(ausgrid[\"date\"].min()) + pd.Timedelta(\"30T\"),\n",
    "        end=pd.Timestamp(ausgrid[\"date\"].max()) + pd.Timedelta(\"1D\"),\n",
    "        freq=\"30T\",\n",
    "    )  # TODO: Change T into Min\n",
    "\n",
    "    # generate a datframe with the time index as index and the customer id as columns\n",
    "    ausgrid_gc_customers = pd.DataFrame(index=time_index,\n",
    "                                        columns=ausgrid[\"Customer\"].unique())\n",
    "    ausgrid_gg_customers = pd.DataFrame(index=time_index,\n",
    "                                        columns=ausgrid[\"Customer\"].unique())\n",
    "    ausgrid_cl_customers = pd.DataFrame(index=time_index,\n",
    "                                        columns=ausgrid[\"Customer\"].unique())\n",
    "\n",
    "    for customer_id in ausgrid[\"Customer\"].unique():\n",
    "        # get the subdataframe\n",
    "        sub_df_gc = ausgrid_gc[ausgrid_gc[\"Customer\"] == customer_id]\n",
    "        sub_df_gg = ausgrid_gg[ausgrid_gg[\"Customer\"] == customer_id]\n",
    "        sub_df_cl = ausgrid_cl[ausgrid_cl[\"Customer\"] == customer_id]\n",
    "\n",
    "        # add the series to the dataframe\n",
    "        tmp_series_gc = parse_date_and_time_and_get_series(sub_df_gc)\n",
    "        if len(tmp_series_gc) != 0:\n",
    "            ausgrid_gc_customers[customer_id] = tmp_series_gc\n",
    "        else:\n",
    "            ausgrid_gc_customers[customer_id] = 0\n",
    "\n",
    "        tmp_series_gg = parse_date_and_time_and_get_series(sub_df_gg)\n",
    "        if len(tmp_series_gg) != 0:\n",
    "            ausgrid_gg_customers[customer_id] = tmp_series_gg\n",
    "        else:\n",
    "            ausgrid_gg_customers[customer_id] = 0\n",
    "\n",
    "        tmp_series_cl = parse_date_and_time_and_get_series(sub_df_cl)\n",
    "        if len(tmp_series_cl) != 0:\n",
    "            ausgrid_cl_customers[customer_id] = tmp_series_cl\n",
    "        else:\n",
    "            ausgrid_cl_customers[customer_id] = 0\n",
    "\n",
    "    # save the dataframes to csv files\n",
    "    ausgrid_gc_customers.to_csv(\n",
    "        \"data/ausgrid_solar_home_dataset/ausgrid_gc_customers.csv\")\n",
    "    ausgrid_gg_customers.to_csv(\n",
    "        \"data/ausgrid_solar_home_dataset/ausgrid_gg_customers.csv\")\n",
    "    ausgrid_cl_customers.to_csv(\n",
    "        \"data/ausgrid_solar_home_dataset/ausgrid_cl_customers.csv\")\n",
    "\n",
    "    # fill every gap in the data with zeroes\n",
    "    ausgrid_cl_customers = ausgrid_cl_customers.fillna(0)\n",
    "    ausgrid_gg_customers = ausgrid_gg_customers.fillna(0)\n",
    "    ausgrid_gc_customers = ausgrid_gc_customers.fillna(0)\n",
    "\n",
    "    # calculate prosumption\n",
    "    # df_final = df_gc + df_cl - df_gg\n",
    "    ausgrid_prosumption = ausgrid_gc_customers + ausgrid_cl_customers - ausgrid_gg_customers\n",
    "\n",
    "    ausgrid_consumption = ausgrid_gc_customers + ausgrid_cl_customers\n",
    "    ausgrid_production = ausgrid_gg_customers\n",
    "\n",
    "    # save the prosumption to a csv file\n",
    "    # name index time\n",
    "    ausgrid_prosumption.index.name = \"time\"\n",
    "    ausgrid_prosumption.to_csv(\n",
    "        \"data/ausgrid_solar_home_dataset/ausgrid_prosumption.csv\")\n",
    "\n",
    "    # generate a facotred version of the prosumption\n",
    "    # load div2\n",
    "    generate_facotred_version_of_ausgrid_dataset(ausgrid_consumption,\n",
    "                                                 ausgrid_production, 0.5, 1,\n",
    "                                                 \"ldiv2\")\n",
    "    # load div5\n",
    "    generate_facotred_version_of_ausgrid_dataset(ausgrid_consumption,\n",
    "                                                 ausgrid_production, 0.2, 1,\n",
    "                                                 \"ldiv5\")\n",
    "    # load 5\n",
    "    generate_facotred_version_of_ausgrid_dataset(ausgrid_consumption,\n",
    "                                                 ausgrid_production, 5, 1,\n",
    "                                                 \"load5\")\n",
    "    # load 2\n",
    "    generate_facotred_version_of_ausgrid_dataset(ausgrid_consumption,\n",
    "                                                 ausgrid_production, 2, 1,\n",
    "                                                 \"load2\")\n",
    "    # pv factor5\n",
    "    generate_facotred_version_of_ausgrid_dataset(ausgrid_consumption,\n",
    "                                                 ausgrid_production, 1, 5,\n",
    "                                                 \"factor5\")\n",
    "    # pv factor10\n",
    "    generate_facotred_version_of_ausgrid_dataset(ausgrid_consumption,\n",
    "                                                 ausgrid_production, 1, 10,\n",
    "                                                 \"factor10\")\n",
    "\n",
    "\n",
    "\n",
    "# main method for setup\n",
    "def setup():\n",
    "    \"\"\"_summary_ : This method sets up the project by downloading the data and creating the necessary folders.\"\"\"\n",
    "    # create a folder named data if it does not exist\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.mkdir(\"data\")\n",
    "    \n",
    "\n",
    "    #Download the Ausgrid Solar Home Dataset\n",
    "    download_ausgrid_solar_home_dataset()\n",
    "    modify_ausgrid_dataset()\n",
    "\n",
    "    # generate the dataset folder\n",
    "\n",
    "    if not os.path.exists(\"data/data_res\"):\n",
    "        os.mkdir(\"data/data_res\")\n",
    "\n",
    "    # generate results opti\n",
    "\n",
    "    if not os.path.exists(\"data/data_res/results_opti\"):\n",
    "        os.mkdir(\"data/data_res/results_opti\")\n",
    "\n",
    "    # generate results forecast\n",
    "\n",
    "    if not os.path.exists(\"data/data_res/results_forecast\"):\n",
    "        os.mkdir(\"data/data_res/results_forecast\")\n",
    "\n",
    "    FACTORS = [\n",
    "    \"results_factor5\", \"results_factor10\" ,\"results_ldiv2\" ,\"results_ldiv5\" ,\"results_load2\" ,\"results_load5\" ,\"results\"\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Generate folder in results_opti and results_forecast for each factor\n",
    "    for factor in FACTORS:\n",
    "        if not os.path.exists(f\"data/data_res/results_opti/{factor}\"):\n",
    "            os.mkdir(f\"data/data_res/results_opti/{factor}\")\n",
    "\n",
    "            #generate optimisation folders \n",
    "\n",
    "            OPTIMISATION = [ \"ds_costs_daily_benchmark\", \"ds_costs_daily_optimisation\", \"imbalance_costs_daily_benchmark\" , \"imbalance_costs_daily_optimisation\", \"results_benchmark\", \"results_optimisation\", \"uni_soe_test\", \"uni_soe_train\"]\n",
    "\n",
    "            for opti in OPTIMISATION:\n",
    "                if not os.path.exists(f\"data/data_res/results_opti/{factor}/{opti}\"):\n",
    "                    os.mkdir(f\"data/data_res/results_opti/{factor}/{opti}\")\n",
    "\n",
    "        if not os.path.exists(f\"data/data_res/results_forecast/{factor}\"):\n",
    "            os.mkdir(f\"data/data_res/results_forecast/{factor}\")    \n",
    "    \n",
    "    print(\"Setup done!\")\n",
    "\n",
    "\n",
    "def generate_gt_files(factor = None, id = 1):\n",
    "\n",
    "    if factor is None:\n",
    "        data_ausgrid_building = pd.read_csv(\"data/ausgrid_solar_home_dataset/ausgrid_prosumption.csv\", parse_dates=[\"time\"], index_col=\"time\").resample(\"1h\").mean()[str(id)]\n",
    "        factor = \"\"\n",
    "    else:\n",
    "        data_ausgrid_building = pd.read_csv(f\"data/ausgrid_solar_home_dataset/ausgrid_prosumption{factor}.csv\", parse_dates=[\"time\"], index_col=\"time\").resample(\"1h\").mean()[str(id)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # shift the data by 0 to 42 hours\n",
    "    list_of_data = []\n",
    "    for i in range(0, 42):\n",
    "        list_of_data.append(data_ausgrid_building.shift(-i))\n",
    "\n",
    "    data_ausgrid_building = pd.concat(list_of_data, axis=1)\n",
    "\n",
    "    data_ausgrid_building.columns = [str(i) for i in range(0, 42)]\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    creation_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    #make parent directory\n",
    "    os.makedirs(f\"data/data_res/results_forecast/results{factor}/{id}/{creation_timestamp}\", exist_ok=True)\n",
    "\n",
    "\n",
    "    # filter only the 00:00:00 time\n",
    "    data_ausgrid_building_daily_train = data_ausgrid_building.between_time(\"00:00:00\", \"00:00:00\")[[str(i) for i in range(0, 24)]].loc[\"2010-07-08\":\"2012-06-30\"].dropna()\n",
    "\n",
    "    # filter only the 12:00:00 time\n",
    "    data_ausgrid_building_noon_train = data_ausgrid_building.between_time(\"12:00:00\", \"12:00:00\")[[str(i) for i in range(0, 42)]].loc[\"2010-07-08\":\"2012-06-28\"].dropna()\n",
    "\n",
    "    data_ausgrid_building_daily_train.to_csv(f\"data/data_res/results_forecast/results{factor}/{id}/{creation_timestamp}/gt_daily_daily.csv\")\n",
    "    data_ausgrid_building_noon_train.to_csv(f\"data/data_res/results_forecast/results{factor}/{id}/{creation_timestamp}/gt_forecast.csv\")\n",
    "\n",
    "    # filter only the 00:00:00 time\n",
    "    data_ausgrid_building_daily_test = data_ausgrid_building.between_time(\"00:00:00\", \"00:00:00\")[[str(i) for i in range(0, 24)]].loc[\"2012-07-08\":\"2013-06-30\"].dropna()\n",
    "\n",
    "    # filter only the 12:00:00 time\n",
    "    data_ausgrid_building_noon_test = data_ausgrid_building.between_time(\"12:00:00\", \"12:00:00\")[[str(i) for i in range(0, 42)]].loc[\"2012-07-08\":\"2013-06-28\"].dropna()\n",
    "\n",
    "    data_ausgrid_building_daily_test.to_csv(f\"data/data_res/results_forecast/results{factor}/{id}/{creation_timestamp}/gt_daily_daily_2..csv\")\n",
    "    data_ausgrid_building_noon_test.to_csv(f\"data/data_res/results_forecast/results{factor}/{id}/{creation_timestamp}/gt_forecast_2..csv\")\n",
    "\n",
    "    print(f\"Generated gt files for factor {factor} and id {id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generates the Ground Truth Files\n",
    "Run the two cells below to generate all needed ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the data folder \n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the ground truth files for every factor place forecasts with properties in the same folder as the gt files ( The properties we forced where different loss functions visible in the paper)\n",
    "\n",
    "FACTORS = [\n",
    "    \"_factor5\", \"_factor10\" ,\"_ldiv2\" ,\"_ldiv5\" ,\"_load2\" ,\"_load5\"\n",
    "\n",
    "]\n",
    "\n",
    "for factor in FACTORS:\n",
    "    for id in range(1, 51):\n",
    "        generate_gt_files(factor, id)\n",
    "\n",
    "\n",
    "for range_id in range(1, 301):\n",
    "    generate_gt_files(None, range_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generates the data needed without the optimisation and the forecasts\n",
    "\n",
    "To continue with data generation you need to generate forecasts of the gt data, this could be done as in the main retraining script but also in other ways.\n",
    "\n",
    "Important: Name them \"result_<focasting_method>_<loss>_.csv\" for train range and \"result_<focasting_method>_<loss>_2..csv\" test range. The format stays the same as in the gt_forecast file. Exemplarily we named our Forecast \"result_FC_with_Cov_MAE_2..csv\". If you differ from our namings you need to adjust things in the dataloaders of the retraining process and the optinet training. You can read in the data comparable to the retraining scripts and extend it with your features. You need at least an MAE and a MSE Forecast and the optimisation for every building to do the retraining process.\n",
    "\n",
    "Afterwards it is important that you run the optimisation script for dataset generation. This ensures that you get optimistation data for the surrogate. For Surrogate Dataset you will need at least an MSE forecasts, named \"result_FC_with_Cov_MSE*\" for train and test. If your naming differs you may need to adjust a lot of paths also in the optimisation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispatchable_feeder_rw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
